---
layout: post
title: 数学之美
category: read
---
<img class="cover" src="/images/2014/5/9787115282828.jpg" />

ISBN: 9787115282828

作者: 吴军

出版社: 人民邮电出版社

出版时间: 2012-5

评价: ☆☆☆☆☆

数学、文字和自然语言一样，都是信息的载体，它们之间原本有着天然的联系。语言和数学的产生都是为了同一个目的——记录和传播信息。

文字的数量和记录一个文明的信息量显然是相关的。然而随着文明的进步，信息量的增加，文字的数量便不再随着文明的发展而增加了，因为没有人能够学会和记住这么多的文字。于是，概念的第一次概括和归类就开始了。在中国，“日”本意是太阳，但是它同时又是一个时间周期，也就是我们讲的一天。这种概念的聚类，在原理上与今天自然语言处理或者机器学习的聚类有很大的相似性。

文字按照意思来聚类，最终会带来一些歧义，也就是有时会弄不清多义字在当前环境下表示的含义。而解决这个问题的方法一直都是依靠上下文，大部分时候可以做到去除歧义。当然，总有个别做不到的时候，这就导致了人们对同一段文字理解的不同。古代不同人对儒家经典的注释和说明，就是按照自己理解消除歧义。今天的情况也类似，对上下文建立的概率模型再好，也有失灵的时候。这些是语言从产生就固有的特点。

文字的载体是石头还是纸张并不重要，它所承载的信息才是最重要的。

无法消除二义性的例子: 此地安能居住，其人好不悲伤
此地-安能-居住，其人-好不-悲伤
此地安-能居住，其人好-不悲伤

很难讲一个准确率在97%的分词器就一定比另一个准确率95%的要好。因为这要看它们选用的所谓正确的人工分词的数据是如何来的。中文分词现在是一个已经解决了的问题，提高的空间微乎其微了。只要采用统计语言模型，效果都查不到哪里去。

在不同的应用中，会有一种颗粒度比另一只更好的情况。比如在机器翻译中，一般来讲颗粒度大翻译效果好。比如“联想公司”作为一个整体，很容易翻译，如果分开成“联想”“公司”，就会翻译失败。但是在另外一些应用中，比如网页搜索，小的颗粒度更好。比如“清华大学”如果作为一个词，用户使用“清华”就找不到了。

自然语言处理的问题其实就是一种通信系统中的解码问题。一般来说编码和解码都是人对人，语音识别就变成人编码，机器解码，其他类似的问题比如从汉语翻译到英语，它们 原理都是一样的。

“汉语信息熵和语言模型的复杂度”，使用信息熵来说明汉语是最简洁的语言。

信息的作用在于消除不确定性，自然语言处理的大量问题就是找相关的信息。

网络爬虫——图论: 广度优先遍历和深度优先遍历

判断两个集合是否相等，最笨的方法是对集合中的元素一一对比，时间复杂度O(N^2)。稍微好一点的是两个集合分别排序，然后循序比较，时间复杂度O(NlogN)。完美的方法是计算这两个集合的指纹，然后直接进行比较，即使用MD5或者SHA-1生成指纹进行比较。网盘的秒传就用到了这个方法。

最大熵原理指出，需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫做“最大熵模型”。我们常说，不要把所有鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素说法，因为当我们遇到不确定性时，就要保留各种可能性。

名词: 
维特比算法
有限状态机
动态规划

延伸阅读:   
第3章: 统计语言模型的工程诀窍。知识背景: 概率论和数理统计。  
第5章: 隐含马尔科夫模型的训练。知识背景: 概率论。  
第6章: 信息论在信息处理中的应用。知识背景: 概率论。  
第10章: PageRank的计算方法。知识背景: 线性代数。  
第11章: TF-IDF的信息论依据。知识背景: 信息论和概率论。  
第12章: 有限状态传感器。知识背景: 图论。  
第14章: 计算向量余弦的技巧。知识背景: 数值分析。  
第15章: 奇异值分解的方法和应用场景。知识背景: 线性代数。  
第16章: 信息指纹的重复性和相似哈希。知识背景: 概率论、组合数学。  
第21章: 个性化的语言模型。知识背景: 概率论。  
第23章: 布隆过滤器的误识别问题。知识背景: 概率论。  
第24章: 贝叶斯网络的训练。知识背景: 概率论。  
第27章: 期望最大化和收敛的必然性。知识背景: 机器学习或者模式分类。  